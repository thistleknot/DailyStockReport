source("vars.r")
#na_interpolation
library(imputeTS)
library(tidyquant)
library(parallel)
library(stringr)

library(car)

#partial least squares
library(pls)

#requires glmnet
#library(parcor)

#PLSC
#library(seminr)
#library(TExPosition)
#tepPLS

library(gtools)

#pcor.test
library(ppcor)
library(glmnet)
'
fil=myNonIndexlists[[1]][1]
data=myNonIndexlists[[1]][2]
size=myNonIndexlists[[1]][3]
'
source("tech_ind.R")

#https://www.datacamp.com/community/tutorials/tutorial-ridge-lasso-elastic-net

# Heteroskedastic Ridge Regression function
hridge <- function(y, X, lambda, weights) {
  #hridge_model <- hridge(y, X, lambda, weights = weights)
  # Use regular ridge regression coefficient as initial values for optimization
  source("vars.r")
  #lambda=.1
  
  #matrix1 <- matrix(1,nrow(X))
  
  #model_init <- glmnet(cbind(matrix1,X), y, alpha = 0, lambda=lambda, standardize = FALSE, family = "binomial", type.measure="deviance",intercept = TRUE)
  #model_init <- cv.glmnet(X, y, alpha = 0, lambda=lambdas_to_try, standardize = FALSE, family = "binomial", type.measure="deviance")
  
  model_init <- glmnet(X, y, alpha = 0, lambda=lambda, standardize = FALSE, family = "binomial", type.measure="deviance")
  
  #a <- glm(as.data.frame(cbind(y,X)),family="binomial")
  
  #b <- glm(y~X-1,family="binomial")
  #coef(a)
  #coef(b)
  #model_init$beta
  #coef(model_init)
  
  
  #as.vector(model_init$beta)
  #as.vector(model_b$coefficients)
  #model_b <- glm(data.frame(cbind(y,X)),family="binomial")
  
  #predict(model_b,data.frame(X),type="response")
  
  #(exp(X%*%model_b$coefficients[-1]+model_b$coefficients[1]) / (1+ exp(X%*%model_b$coefficients[-1]+model_b$coefficients[1])))
  
  #values <- rowSums(X*model_b$coefficients[-1])+model_b$coefficients[1]
  #fitted <- 1/1+exp(-values)
  
  #predict(model_init, X, type="response")
  
  #betas_init <- as.vector(model_init$beta)
  betas_init <- as.vector(coef(model_init))
  
  # Solve optimization problem to get coefficients
  
  # Heteroskedastic Ridge Regression loss function - to be minimized
  hridge_loss <- function(betas) {
    #delta predicted y squared 
    #deviance(model_init) + lambda * sum(weights * betas^2)
    #sum((y - round(X %*% betas))^2) + lambda * sum(weights * betas^2)
    #(1-caret::confusionMatrix(as.factor(ifelse((X %*% betas)>.5,1,0)), as.factor(unlist(y)))$overall["Accuracy"])^2 + lambda * sum(weights * betas^2)
    #http://christopher5106.github.io/deep/learning/2016/09/16/about-loss-functions-multinomial-logistic-logarithm-cross-entropy-square-errors-euclidian-absolute-frobenius-hinge.html
    #fitted <- X %*% betas
    
    #fitted <- (exp(X%*%betas) / (1+ exp(X%*%betas)))
    
    fitted <- (exp(X%*%betas[-1]+betas[1]) / (1+ exp(X%*%betas[-1]+betas[1])))
    
    fitted[fitted==1] <- .99
    fitted[fitted==0] <- .01
    
    #fitted <- (exp(X%*%betas_init) / (1+ exp(X%*%betas_init)))
    
    #(exp(cbind(matrix1,X)%*%model_init$beta) / (1+ exp(cbind(matrix1,X)%*%model_init$beta)))
    
    #fitted <- (exp(cbind(matrix1,X)%*%model_b$coefficients) / (1+ exp(cbind(matrix1,X)%*%model_b$coefficients)))
    
    #print(betas)
    #sum(log(1+exp(-((y - round(X %*% betas))^2)))) + lambda * sum(weights * betas^2)
    
    #lossFunction <- (sum(-y*log(fitted)+(1-y)*log(1-fitted)))^2
    lossFunction <- (sum(-y*log(fitted)+(1-y)*log(1-fitted)))
    #if(FALSE)
    #{
    #if(lossFunction=="NaN")
    #   {
    #     lossFunction=100000}
    #if(!lossFunction=="NaN")
    #{lossFunction=lossFunction}
    #}
    
    value <- lossFunction + lambda * sum(weights * betas[-1]^2)
    #value <- lossFunction + lambda * sum(weights * betas^2)
    
    value
    
    #sum(-y*log(fitted)+(1-y)*log(1-fitted)) + lambda * sum(weights * betas_init^2)
    
    #sum((-y*log(fitted)-(1-y)*log(fitted))^2) + lambda * sum(weights * betas^2)
    
    #sum(log(1+exp(-(y - round(X %*% betas))))) + lambda * sum(weights * betas^2)
    #log(1+exp(-(sum((y - X %*% betas))))) + lambda * sum(weights * betas^2)
  }
  
  #coef <- optim(betas_init[-1], hridge_loss)$par
  coef <- optim(betas_init, hridge_loss)$par
  # Compute fitted values and multiple R-squared: matrix multiplication
  #fitted <- X %*% coef
  #fitted <- (exp(X%*%coef+betas_init[1]) / (1+ exp(X%*%coef+betas_init[1])))
  fitted <- (exp(X%*%coef[-1]+coef[1]) / (1+ exp(X%*%coef[-1]+coef[1])))
  
  #might want to adjust for binomial loss function that uses a log
  #http://christopher5106.github.io/deep/learning/2016/09/16/about-loss-functions-multinomial-logistic-logarithm-cross-entropy-square-errors-euclidian-absolute-frobenius-hinge.html
  MAPE = 1-caret::confusionMatrix(as.factor(ifelse(fitted>.5,1,0)), as.factor(unlist(y)))$overall["Accuracy"]
  
  h_ssr <- t(y - fitted) %*% (y - fitted)
  rsq <- cor(y, fitted)^2
  names(coef[-1]) <- colnames(X)
  output <- list("MAPE" = MAPE,
                 #"intercept" = betas_init[1],
                 "coef" = coef,
                 "fitted" = fitted,
                 "rsq" = rsq,
                 "ssr" = h_ssr,
                 "lambda" = lambda)
  return(output)
}

put_symbols_into_file <- function(fil,data,size,first.date,last.date) {
  set <- Filter(Negate(anyNA),batch_get_symbols_size(data,size))
  length(set)
  #names(set) <- data
  #https://stackoverflow.com/questions/51598172/remove-an-element-from-a-list-that-contains-only-na
  #names(Filter(Negate(anyNA),set))
  dput(set,fil)
}

#https://gist.github.com/rhochreiter/7029236
na.omit.list <- function(y) { return(y[!sapply(y, function(x) all(is.na(x)))]) }

foldRecords <- function(data,numFolds=5) {
  
  lengthOfRecords = nrow(data.frame(data))
  
  #had to visualize 5 folds in excel and looked at how the padding on left and right were setup each row and modelled it.
  #16%
  rightPadding <- lengthOfRecords/numFolds*2
  leftIncreaseAmount <- lengthOfRecords/numFolds/2
  
  records <- mclapply((0:(numFolds-1)),function(x)
  {
    ((round((leftIncreaseAmount*x+1)):(lengthOfRecords-round(rightPadding-leftIncreaseAmount*x))))
  })
  return(records)
}

betaTestCoefficient=1
# eventually move betaTestCoefficient to the function call itself
batch_get_symbols_size <- function(data,size,first.date,last.date) {
  subset_data <- sort(sample(unlist(data),as.integer(size*betaTestCoefficient),replace=FALSE))
  names(subset_data) <- subset_data
  
  set <- mclapply(subset_data, function (x)
  {
    tq_get(x, from = first.date, to=last.date)
  })
  #already named
  #names(set) <- subset_data
  return(set)
}

batch_get_symbols <- function(symbols, first.date= (Sys.Date() - 821), last.date=Sys.Date()) {
  subset_data <- sort(sample(unlist(symbols),replace=FALSE))
  names(subset_data) <- subset_data
  
  set <- mclapply(subset_data, function (x)
  {
    tq_get(x, from = first.date, to=last.date)
  })
  #already named
  #names(set) <- subset_data
  return(set)
}

#fil=files[[1]][1]
#returns symbol names
filtered_symbols <- function(fil)
{
  #sort(names(Filter(Negate(anyNA),dget(fil, keep.source = TRUE))))
  sort(names(dget(fil, keep.source = TRUE)))
  #list <- dget(fil, keep.source = TRUE)
  #names(list) <- list
  #dget(fil, keep.source = TRUE)$df.control$ticker
  #return(names(list))
}

#joins list of symbols to dput file name
join_file_symbols <- function(x)
{
  list(filList[[x]],filteredSymbols[[x]])
}

join_file_csvNames <- function(x)
{
  list(filList[[x]],csvNames[[x]])
}

write_subset_csv <- function(fil,name)
{
  fwrite(dget(fil[[1]][[1]], keep.source = TRUE)$df.tickers, name) 
}

#files=All[[2]]

#files[[1]] is source
put_adjusted_into_file <- function(files)
{
  
  #takes list
  #fil <- filteredSymbolsList[[1]][[1]]
  
  source_data <- dget(files[[1]], keep.source = TRUE)
  #symbols <- unique(source_data$df.tickers$ticker)
  
  #split dataframe into list
  #list_source <- group_split(source_data$df.tickers, source_data$df.tickers$ticker)
  #symbols <- sort(unique(source_data$df.tickers$ticker))
  
  #used when list (saves time later on list_source_data if working with dataframe to keep this name)
  list_source <- source_data
  symbols <- names(source_data)
  
  #names(list_source) <- symbols
  names(symbols) <- symbols
  
  #colnames(source_data[[2]][,])
  #colNames= c("Open", "High", "Low", "Close", "Volume", "Adjusted", "Date", "Ticker", "Return.Adjusted", "Return.Closing","Ticker2")
  colNames= c("Symbol","Date","Open","High","Low","Close","Volume","Adjusted")
  
  #x=1
  
  list_source_data <- mclapply(symbols, function (x) {
    setNames(list_source[[x]][,], colNames)
  })
  
  dput(mclapply(symbols, function(x) {
    as.data.frame(adjustOHLC(xts(as.data.frame(list_source_data[[x]][,])[,c("Open","High","Low","Close","Volume","Adjusted")],order.by=as.POSIXct(list_source_data[[x]][,]$Date)), adjust=c("split","dividend"), symbol.name=symbols[x], use.Adjusted=TRUE))
  }),files[[2]])
  
  print(files[[2]])
  
}

#files=All2[[1]]
put_to_file_bind_dates <- function(files)
{
  date_data <- dget(files[[1]], keep.source = TRUE)
  adjusted_data <- dget(files[[2]], keep.source = TRUE)
  output_data <- files[[3]]
  
  #list <- group_split(date_data$df.tickers, date_data$df.tickers$ticker)
  #list <- group_split(date_data$df.tickers, date_data$df.tickers$ticker)
  list <- date_data
  #list_names <- sort(unique(date_data$df.tickers$ticker))
  list_names <- names(list)
  
  #names(list) <- list_names
  names(list_names) <- list_names
  
  #x=list_names[1]
  dput(mclapply(list_names,function (x) {
    #cbind("Date"=list[[x]][,]$ref.date,adjusted_data[[x]][,])
    cbind("Date"=list[[x]][,]$date,adjusted_data[[x]][,])
  }),files[[3]])
}

#files=All3[[2]]
write_csvs <- function(files)
{
  date_data <- dget(files[[1]], keep.source = TRUE)
  fwrite(rbindlist(date_data, use.names=TRUE, fill=TRUE, idcol="Symbol"),files[[2]])
}

#files = Indexes[1]
clean_indexes <- function(files)
{
  temp <- Filter(Negate(anyNA),dget(files[[1]],keep.source = TRUE))
  dput (temp,files[[1]])
}

extract_year <- function (x_date)
{
  #https://stackoverflow.com/questions/36568070/extract-year-from-date
  substring(x_date,1,4) #This takes string and only keeps the characters beginning in position 7 to position 10
}

#symbol <- selected[4]
#called by derive_Betas
derive_Back <- function (symbol)
{
  derived_start_date <- FiveYearsBack
  
  repeat {
    acquired <- tq_get(symbol, from=derived_start_date, to=derived_start_date+7)
    if (is.na(acquired) ){
      derived_start_date = derived_start_date + months(3)
    }
    if (!is.na(acquired))
    {
      break
    }
  }
  
  acquired <- data.frame(tq_get(symbol, from=derived_start_date, to=last.date))
  names(acquired) <- c("Symbol","Date","Open","High","Low","Close","Volume","Adjusted")
  rownames(acquired) <- acquired$date
  adjusted <- adjustOHLC(xts(as.data.frame(acquired)[,c("Open","High","Low","Close","Volume","Adjusted")],order.by=as.POSIXct(acquired$Date)),symbol.name=symbol,adjust=c("split","dividend"), use.Adjusted=TRUE)
  
  #names(adjusted) <- c("Date","Open","High","Low","Close","Volume","Adjusted")
  
  returnSet <- list(adjusted,derived_start_date)
  return(returnSet)
}

derive_Betas <- function(symbol)
{
  returnSet <- derive_Back(symbol)
  
  adjusted <- returnSet[[1]]
  derived_start_date <- as.Date(returnSet[[2]])
  
  xreturns <- as.data.frame(CalculateReturns(adjusted[,c("Adjusted"),drop=FALSE]))
  
  #View(xreturns)
  
  #https://stackoverflow.com/questions/29511215/convert-row-names-into-first-column
  xreturns <- setDT(xreturns, keep.rownames = "Date")[]
  colnames(xreturns) <- c("Date","Returns")
  
  xreturns[duplicated(xreturns$Date),]
  
  #https://r.789695.n4.nabble.com/as-date-do-not-know-how-to-convert-mydata-1-to-class-quot-Date-quot-td4638691.html
  #convert character column to date
  #had to do a lapply(xreturns,class) to see mismatched classes
  xreturns[,1] <- as.Date(paste(xreturns$Date), "%Y-%m-%d")
  
  start_date <- xreturns$Date[1]
  
  Beta_trading_dates <- data.frame(trading_dates[trading_dates >= paste(start_date) & trading_dates <= paste(last.date),])[,,drop=FALSE]
  colnames(Beta_trading_dates) <- c("Date")
  
  Beta_trading_dates[duplicated(Beta_trading_dates$Date),]
  #rownames(trading_dates) <- c(trading_dates)
  
  #needed data.frame(xreturns) else would result in NA's
  
  #left join
  test<-merge.data.frame(x=Beta_trading_dates[,"Date",drop=F],y=xreturns[,c("Date","Returns"),drop=F],by="Date",all.x=T)
  
  test[duplicated(test$Date),]
  
  xreturns<-na_interpolation(test)
  
  rownames(xreturns) <- xreturns$Date
  
  xreturns[duplicated(xreturns$Date),]
  
  start_date <- rownames(xreturns)[1]
  
  yreturns <- indices_Returns[indices_Returns$Date >= start_date,]$X.SP500TR
  head(xreturns)
  head(yreturns)
  tail(xreturns)
  tail(yreturns)
  head(Beta_trading_dates)
  tail(Beta_trading_dates)
  nrow(xreturns)
  nrow(yreturns)
  nrow(Beta_trading_dates)
  
  #chop off first na return
  #View(tail(xreturns,-1))
  
  #View(yreturns)
  
  #x<-as.xts(eod_ret[,selected])
  #y<-as.xts(eod_ret[,"X.SP500TR",drop=F]) #benchmark
  #linearModTotal <- lm(y~x)
  
  #View(yreturns[duplicated(yreturns$Date),])
  rownames(yreturns) <- yreturns$Date
  #not sure why I don't need to remove the 1st element, but it seems I have an adjusted return?
  #y<-.xts(tail(yreturns$Return,-1),order.by=as.POSIXct(yreturns$Date))
  #x<-as.xts(tail(xreturns$Return,-1),order.by=as.POSIXct(xreturns$Date[,dro]))
  
  #x<-as.xts(eod_ret[,selected])
  #y<-as.xts(eod_ret[,"X.SP500TR",drop=F]) #benchmark
  #linearModTotal <- lm(y~x)
  #Betas <- linearModTotal$coefficients[order(linearModTotal$coefficients)]
  
  x <- as.xts(xreturns[,2,drop=FALSE])
  y <- as.xts(yreturns[,2,drop=FALSE])
  linearModTotal <- lm(y~x)
  
  returnSet <- list(linearModTotal$coefficients,start_date)
  return(returnSet)
}

colMax <- function(data) sapply(data, max, na.rm = TRUE)

MAPE_backstep <- function(combined)
{#combined=rebalanced
  #combined=total_combined
  #combined=combined_2ndPass
  #traceback()
  #https://stackoverflow.com/questions/9951319/dynamic-column-name-in-for-loop-with-cbind
  #colnames(combined)[1] <- paste(y)
  
  source("vars.r")
  set.seed(11)
  
  v_folds=sample(rep(1:numFolds, length=nrow(as.data.frame(combined))))
  
  #print(numFolds)
  
  y=colnames(combined[1])
  
  RemoveColumnFlag = "searching"
  removedSet <- c()
  
  #used to hold formula's
  formulaHolder <- c()
  columnHolder <- c()
  
  while(RemoveColumnFlag!="")
  {
    print(RemoveColumnFlag)
    if (RemoveColumnFlag == "searching")
    {
      removedSet = c()
      reducedSet <- combined
      
    }
    if (RemoveColumnFlag != "searching")
    {
      removedSet = c(removedSet,RemoveColumnFlag)
      #could just iteratively remove one at a time vs this removedSet... but having a list of what I remove is useful
      reducedSet <- dplyr::select(combined,-all_of(c(removedSet)))
      
      #View(formulaHolder)
      formulaHolder <- rbind(formulaHolder,t(c(y,ncol(reducedSet)-1,bestMAPE,paste(RemoveColumnFlag,collapse=","),
                                               paste(colnames(reducedSet),collapse=","))))
      columnHolder <- c(columnHolder,list(colnames(reducedSet)))
      
      colnames(formulaHolder) <- c("y","numCol","MAPE","removed columns","formula")
      
      #reducedSet <- dplyr::select(combined,-all_of(removedSet))
    }
    
    cnames <- colnames(reducedSet)
    colScores=matrix(NA,ncol(reducedSet))
    
    #can't parallelize
    
    colScores <- matrix(unlist(mclapply(1:ncol(reducedSet), function(nameCol)
    {#nameCol=1
      
      #parse dataSet
      firstColumnY = reducedSet[,1,drop=FALSE]
      nonY <- reducedSet[-nameCol]
      nonY <- nonY[,-1]
      
      #full
      if(nameCol==1)
      {
        reducedSet2 <- reducedSet
      }
      
      if(nameCol!=1)
      {
        reducedSet2 <- cbind(firstColumnY,nonY)
      }
      
      if(ncol(reducedSet2)==1)
      {
        RemoveColumnFlag=""
        #didn't remove anything
        bestMAPE= min(na.omit(colScores[1:length(colScores)]))
        formulaHolder <- rbind(formulaHolder,c(y,ncol(reducedSet)-1,bestMAPE,paste(RemoveColumnFlag,collapse=","),paste(colnames(reducedSet[,-1]),collapse=",")))
        mape.cv = bestMAPE
        #break
      }
      #print("cv")
      if(!ncol(reducedSet2)==1)
      {
        #colScores is mclapply, doubling up on that extrapolates the threads too much and breaks the process.  Example 100 columns = 100 threads * 5, is 500 threads!
        #I've had it freeze on 24 columns...  But it makes most sense to have this be not multithreaded (outer loop vs inner loop, or at the very least, only have 1 multithreaded)
        errors <- lapply(1:numFolds, function(k)
        {#k=1
          t <- reducedSet2[which(v_folds!=k),,drop=FALSE]
          #print(t)
          v <- reducedSet2[which(v_folds==k),,drop=FALSE]
          #print(v)
          
          model <- glm(t,family="binomial")
          #print(model)
          
          predV <- c()
          predV <- predict(model, v[-1])
          
          predT <- c()
          predT <- model$fitted.values
          
          #numOfItems = length(pred)
          
          actualT <- c()
          actualT <- t[1]
          
          actualV <- c()
          actualV <- v[1]
          #cv.errors[k]=100*1/numOfItems*sum(abs((actual-pred)/pred))
          #return(100*1/numOfItems*sum(abs((actual-pred)/pred)))
          
          #levels(as.factor(ifelse(pred > 0.5, 1, 0)))
          #levels(as.factor(unlist(actual)))
          
          optimal <- InformationValue::optimalCutoff(actualT, predT, optimiseFor = "misclasserror", returnDiagnostics = FALSE)
          #optimal.cutpoints(leadSubset, model$fitted.values,methods="MaxSp")
          #accuracy <- confusionMatrix(as.factor(unlist(ifelse(model$fitted.values > optimal, 1, 0))), as.factor(unlist(leadSubset)))
          
          #fit <- as.factor(as.integer(as.character(ifelse(model$fitted.values>optimal,1,0))))
          #act <- as.factor(as.integer(as.character(unlist(leadSubset))))
          #accuracy <- caret::confusionMatrix(fit,act)
          
          accuracy <- caret::confusionMatrix(as.factor(ifelse(predV > optimal, 1, 0)),as.factor(actualV[,1]))
          
          #both
          MAPE <- 1-accuracy$overall["Accuracy"]
          
          #sensitivity
          #MAPE <- 1-accuracy$byClass[1]
          
          #return(mean(abs((actual-pred)/pred)))
          return(MAPE)
          #},mc.cores = 2)
        })
        
        mape.cv=mean(unlist(errors))
        
        #these are scores of errors without a given nameCol
        #find lowest score, and remove the nameCol
        #colScores[nameCol]=mape.cv
        mape.cv
        
      }
      #print("endcv")
      mape.cv
      
    })))
    #start at 2, becuase 1 is full model, 2 is when we start removing factors
    
    # if above condition: if(ncol(reducedSet2)==1) was NOT triggered
    if(!RemoveColumnFlag=="")
    {
      
      bestMAPE = min(colScores[2:length(colScores)])
      
      #print(paste("Removed: ",RemoveColumnFlag))
      print(ncol(reducedSet)-1)
      print(bestMAPE)
      
      #don't remove y  
      if(ncol(reducedSet)<=2)
        #if (nameCol == 1)
      {
        #RemoveColumnFlag =="searching"
        RemoveColumnFlag = ""
        #formulaHolder <- rbind(formulaHolder,c(y,ncol(reducedSet)-1,bestMAPE,paste(RemoveColumnFlag,collapse=","),paste(colnames(dplyr::select(reducedSet,-all_of(c(RemoveColumnFlag)))),collapse=",")))
      }
      #if (nameCol != 1)
      if(ncol(reducedSet)>2)
      {
        #I remove min, because it's that set that had min score and the set is named after the removed column (i.e. cnames reference), add 1 due to starting at 2
        #second set is 2:, first set is 1:up, so it matches 2 to first
        RemoveColumnFlag = cnames[1:length(cnames)][(which(colScores[2:length(colScores)] == bestMAPE))+1]
        #breaking with interactions that contain non lagged y, this was to prevent it from removing y
        #RemoveColumnFlag = RemoveColumnFlag[!str_detect(RemoveColumnFlag,y)]
        #if(RemoveColumnFlag==y)
        {
          #break
        }
        #print(paste("Removing:",RemoveColumnFlag))
        
        #formulaHolder <- rbind(formulaHolder,c(y,length(colnames(dplyr::select(reducedSet,-all_of(c(RemoveColumnFlag)))))-1,bestMAPE,paste(RemoveColumnFlag,collapse=","),paste(colnames(dplyr::select(reducedSet,-all_of(c(RemoveColumnFlag)))),collapse=",")))
        
        #columnHolder <- c(columnHolder,list(colnames(dplyr::select(reducedSet,-all_of(c(RemoveColumnFlag))))))
        
        #colnames(formulaHolder) <- c("y","numCol","MAPE","removed columns","formula")
        
      }
      if(length(RemoveColumnFlag)>=ncol(reducedSet)-1)
      {
        RemoveColumnFlag = ""
      }
      print(paste("Removing:",RemoveColumnFlag))
      #formulaHolder <- rbind(formulaHolder,c(y,ncol(reducedSet2)-1,bestMAPE,paste(RemoveColumnFlag,collapse=","),paste(colnames(reducedSet2),collapse=",")))
      
      #if((ncol(reducedSet2)-1)==1)
      #if(RemoveColumnFlag="")
      {
        #RemoveColumnFlag =="searching"
        #break
      }
      
      #print(RemoveColumnFlag)
    }
    
  }
  
  factors <- formulaHolder[,2]
  factormax <- formulaHolder[,2][1]
  #View(formulaHolder)
  #plot(factors,formulaHolder[,3], main=y, xlab="#Factors", ylab="MAPE",las=1, col="steelblue", xlim=c(as.double(factormax), 0),pch=20)
  
  #breaks on ETH
  #above0 <- data.frame(formulaHolder[as.double(formulaHolder[,2])>0])
  
  #above0 <- data.frame((formulaHolder[as.double(formulaHolder[,2])>0,-4]))
  
  above0 <- data.frame(formulaHolder[as.double(formulaHolder[,2])>0,,drop=FALSE])
  #above0 <- as.data.frame(formulaHolder[formulaHolder[,2]>58,])
  
  #View(above0)
  #if size one, it doesn't convert it to rows properly
  #if(nrow(t(above0))==1)
  #{
  #above0 <- t(above0)
  #}
  #View(above0)
  #minMAPEscore <- data.frame(t(formulaHolder[as.double(formulaHolder[,3])==min(as.double(formulaHolder[,3])),-4]))
  minMAPEscore <- data.frame(t(formulaHolder[as.double(above0[,3])==min(as.double(above0[,3])),-4]))
  #View(minMAPEscore)
  
  best_columnSet <- (columnHolder[which(as.double(formulaHolder[,3])==min(as.double(formulaHolder[,3])))])
  #View(columnHolder)
  
  #View(formulaHolder)
  
  #plot(formulaHolder[,"MAPE"])
  
  plot(factors,formulaHolder[,"MAPE"], main=y, xlab="#Factors", ylab="MAPE",las=1, col="steelblue", xlim=c(as.double(factormax), 0),pch=20)
  
  #if a list of 2 (Vs a length of 4) return last element in list
  if(nrow(minMAPEscore)!=1)
  {
    minMAPEscore <- data.frame(t(minMAPEscore[length(minMAPEscore)]))
    best_columnSet <- data.frame(t(best_columnSet[length(best_columnSet)]))
    View(best_columnSet)
  }
  
  #always return last element as it's smallest n
  
  #threshold <- min(as.double(formulaHolder[,3]))+sd(as.double(formulaHolder[,3]))
  #for now skipping this
  #if(is.na(threshold))
  #{
  
  return(list(minMAPEscore,best_columnSet))
  #}
  
  #less_than_threshold <- formulaHolder[as.double(formulaHolder[,3])<=threshold,-4]
  #minK1SDEVMin <- less_than_threshold[which(as.double(less_than_threshold[,2])==min(as.double(less_than_threshold[,2]))),]
  #if too little information to do an sdev comparison
  
  #return(minK1SDEVMin)
  #View(formulaHolder)
}

back_step_partial_regression_analysis <- function(data)
{
  #https://www.displayr.com/using-partial-least-squares-to-conduct-relative-importance-analysis-in-r/
  y=colnames(data[,1])
  
  f <- as.formula(
    paste(y, 
          paste(sprintf("`%s`", colnames(data[,-1]))
                , collapse = " + "), 
          sep = " ~ "))
  
  pls.model <- plsr(f, data = data, validation= "CV")
  
  # Find the number of dimensions with lowest cross validation error
  cv = RMSEP(pls.model)
  best.dims = which.min(cv$val[estimate = "adjCV", , ]) - 1
  
  # Rerun the model
  pls.model = plsr(f, data = data, ncomp = best.dims)
  
  coefficients = names(coef(pls.model)[,1,1])
  #sum.coef = sum(sapply(coefficients, abs))
  #coefficients = coefficients * 100 / sum.coef
  #coefficients = sort(coefficients[, 1 , 1])
  #barplot((coefficients))
  
  return(data[colnames(pls.model$model)])
  
  #pls.net(data, scale = TRUE, k = 10, ncomp = best.dims,verbose=FALSE)
  
}

back_step_partial_correlation <- function(data)
{
  
  #set.seed(11)
  source("vars.r")
  
  #data=data=indices_
  
  #https://www.researchgate.net/post/Do_we_need_to_standardize_variables_with_different_scales_before_doing_correlation_analysis
  #no need to standardize
  #noYScaledData <- scale(data[,-1],center = TRUE, scale = TRUE)
  
  if((ncol(data)-1)>(nrow(data)*.8))
  {
    while((ncol(data)-1)>(nrow(data)*.8))
    {
      sets <- return_Reduced_Sets(data)
      #end if condition
      
      #1 pass
      splitPCS <- mclapply(sets,function(z)
      {#z=sets[1]
        data=data.frame(z) 
        
        back_step_partial_correlation_inner(data)
        
      })
      
      data <- cbind(data[,1],do.call(cbind,mclapply(splitPCS, function(b){b[-1]})))
      ncol(data)
      #end while
    }
    
  }
  
  back_step_partial_correlation_inner(data)
  
  #pcor_Scores_final <- mclapply(2:length(colnames(data)), function(p)
  #{#p=2
  #  y=data[1]
  #  xsfiltered <- data[c(-1,-p)]
  #  control <- data[p]
  #  pcor.test(y,control,xsfiltered)
  
  #})
  #print(pcor_Scores_final)
  #print(colnames(data))
  
  #return(data)
}

back_step_partial_correlation_inner <- function(innerdata)
  
{#innerdata=data
  stop= 0
  
  n=nrow(innerdata)
  threshold_t <- qt(.05, n-2, lower.tail = TRUE, log.p = FALSE)
  
  while(stop==0)
  {
    
    if(length(colnames(innerdata))<=3)
    {
      break
    }
    
    internal_Scores <- mclapply(2:length(colnames(innerdata)), function(p)
    {#p=2
      print(p)
      y=innerdata[,1]
      xsfiltered <- innerdata[c(-1,-p)]
      control <- innerdata[p]
      
      #manual
      #print(p)
      print(colnames(innerdata[p]))
      
      source("vars.r")
      
      f1 <- as.formula(
        paste(colnames(y), 
              paste(sprintf("`%s`", colnames(xsfiltered))
                    , collapse = " + "), 
              sep = " ~ "))
      
      m1 <- plsr(f1, data = cbind(y,xsfiltered), validation= "CV",segments = 2)
      
      f2 <- as.formula(
        paste(colnames(control), 
              paste(sprintf("`%s`", colnames(xsfiltered))
                    , collapse = " + "), 
              sep = " ~ "))
      
      m2 <- plsr(f2, data = cbind(control,xsfiltered), validation= "CV",segments = 2)
      
      # Find the number of dimensions with lowest cross validation error
      cv1 = RMSEP(m1)
      cv2 = RMSEP(m2)
      best.dims1 = which.min(cv1$val[estimate = "adjCV", , ]) - 1
      print(best.dims1)
      best.dims2 = which.min(cv2$val[estimate = "adjCV", , ]) - 1
      print(best.dims2)
      
      # Rerun the model
      m1 = plsr(f1, data = cbind(y,xsfiltered), ncomp = best.dims1)
      
      m2 = plsr(f2, data = cbind(control,xsfiltered), ncomp = best.dims2)
      
      #m1 <- lm(cbind(y,xsfiltered))
      #m2 <- lm(cbind(control,xsfiltered))
      
      abs(cor(data.frame(m1$residuals)[,best.dims1],data.frame(m2$residuals)[,best.dims2]))
      
    })
    
    #I'm removing min correlation between x and y controlling for rest of data
    #I want the relationship to be significant when comparing with y.
    remove_scor <- min(unlist(mclapply(internal_Scores, `[[`, 1)))
    
    t = remove_scor * sqrt(n-2)/ sqrt(1-remove_scor^2)
    
    sig <- dt(t, n-2, log = FALSE)
    
    print(sig)
    
    removeColumn <- which(
      (
        unlist(mclapply(internal_Scores, `[[`, 1))
        #add 1 because data has 1st element as y
      )==remove_scor)+1
    print(removeColumn)
    
    if(!is.na(sig))
    {
      if(sig<=.05)
      {
        stop=1
        break
      }
    }
    
    print(removeColumn)
    innerdata <- innerdata[-removeColumn]
    print(colnames(innerdata))
    
  }
  
  return(innerdata)
}

return_Reduced_Sets <- function(recieveddata)
{#recieveddata<-data
  #print(targetColumnSize)
  
  #rescale using normalization
  ncolMinus1 <- recieveddata[,-1]
  
  #factorBy = nrow(data)/ncolMinus1
  loops = ceiling(ncol(ncolMinus1)/(nrow(recieveddata)/5)-2)-1
  size <- (ncol(ncolMinus1)/loops)
  
  ncol(recieveddata)
  #round(size*2)
  
  #set.seed(11)
  source("vars.r")
  
  colSets = sample(1:ncol(ncolMinus1), ncol(ncolMinus1), replace=F)
  sets <- mclapply(seq(1:loops),function(i)
  {
    #i=1
    first <- round(((i-1)*size+1))
    print(first)
    
    last=round(i*size)
    print(last)
    #last <- (i*round(size))
    
    #print()
    cbind(recieveddata[,1],ncolMinus1[,c(colSets[first:last])])
    
  })
  return(sets)
}

back_step_vif <- function(data)
{
  #scaleddata <- scale(data, center=TRUE, scale=TRUE)
  
  scaledData <- data.frame(scale(data,center=TRUE, scale=TRUE))
  
  model <- lm(scaledData)
  
  #remove aliased
  #https://stackoverflow.com/questions/28885160/vifs-returning-aliased-coefficients-in-r
  ld.vars <- attributes(alias(model)$Complete)$dimnames[[1]]
  
  print(ld.vars)
  
  nonAliasedData <- scaledData[!colnames(scaledData) %in% ld.vars]
  
  newData <- nonAliasedData
  
  maxVif = 10
  
  while(maxVif >= 10)
  {
    
    fit.new <- lm(newData)
    
    vifs <- vif(fit.new)
    
    maxVif <- max(vifs)
    while(is.na(maxVif))
    {
      #sloppy non recursive way
      removeColumns <- colnames(newData)[findCorrelation(cor(newData))]
      newData <- dplyr::select(newData, -all_of(removeColumns))
      fit.new <- glm(newData,family="binomial",control = list(maxit = ncol(newData)))
      
      vifs <- vif(fit.new)
      maxVif <- max(vifs)
      
    }
    print(maxVif)
    remove <- names(which(vifs==maxVif))
    print(remove)
    
    newData <- dplyr::select(newData,-all_of(remove))
    
  }
  #print(colnames(newData))
  print(vifs)
  
  return(dplyr::select(data,all_of(colnames(newData))))
  
}

back_step_vif_binomial <- function(data)
{#data=rebalanced
  #scaleddata <- scale(data, center=TRUE, scale=TRUE)
  
  #scaledData <- data.frame(cbind(data[1],scale(data[-1],center=TRUE, scale=TRUE)))
  
  scaledData <- data
  
  model <- glm(scaledData,family=binomial)
  
  #remove aliased
  #https://stackoverflow.com/questions/28885160/vifs-returning-aliased-coefficients-in-r
  ld.vars <- attributes(alias(model)$Complete)$dimnames[[1]]
  
  print(ld.vars)
  
  nonAliasedData <- scaledData[!colnames(scaledData) %in% ld.vars]
  
  newData <- nonAliasedData
  
  maxVif = 10
  
  #library(corrplot)
  
  #Will return NaN if variables are too closely correlated
  
  while(maxVif >= 10)
  {
    
    fit.new <- glm(newData,family="binomial",control = list(maxit = ncol(newData)))
    
    vifs <- vif(fit.new)
    
    maxVif <- max(vifs)
    while(is.na(maxVif))
    {
      #sloppy non recursive way
      removeColumns <- colnames(newData)[findCorrelation(cor(newData))]
      newData <- dplyr::select(newData, -all_of(removeColumns))
      fit.new <- glm(newData,family="binomial",control = list(maxit = ncol(newData)))
      
      vifs <- vif(fit.new)
      maxVif <- max(vifs)
      
    }
    print(maxVif)
    remove <- names(which(vifs==maxVif))
    print(remove)
    
    newData <- dplyr::select(newData,-all_of(remove))
    
  }
  #print(colnames(newData))
  print(vifs)
  
  return(dplyr::select(data,all_of(colnames(newData))))
  
}